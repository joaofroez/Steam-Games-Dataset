# Steam-Games-Dataset
Reposit√≥rio dedicado ao controle de versionamento do Banco de Dados "[Steam Games Dataset](https://www.kaggle.com/datasets/fronkongames/steam-games-dataset)".


# üéÆ Steam ETL Pipeline

Este projeto consiste em um pipeline de Engenharia de dados completo para ingest√£o, processamento, importa√ß√£o e an√°lise do dataset de jogos da Steam. O sistema envolve a leitura de dados brutos, normaliza√ß√£o para um banco transacional (OLTP), cria√ß√£o de automatiza√ß√µes no PostgreSQL e transforma√ß√£o final para um Data Warehouse (OLAP/Star Schema).

![Badge Airflow](https://img.shields.io/badge/Orchestration-Apache%20Airflow-blue?style=for-the-badge&logo=apacheairflow)
![Badge Docker](https://img.shields.io/badge/Container-Docker-2496ED?style=for-the-badge&logo=docker)
![Badge Postgres](https://img.shields.io/badge/Database-PostgreSQL-336791?style=for-the-badge&logo=postgresql)
![Badge Python](https://img.shields.io/badge/Code-Python-3776AB?style=for-the-badge&logo=python)

---

## üèóÔ∏è Arquitetura do Projeto

O pipeline foi desenhado para ser **reproduz√≠vel** via Docker.

1.  **Extract (Extra√ß√£o):** Leitura de arquivo `JSON` (Steam Games Dataset) utilizando processamento em stream (`ijson`) para alta performance.
2.  **Transform (Transforma√ß√£o):**
    * Limpeza de dados (Data Cleaning) e tratamento de tipos.
    * Normaliza√ß√£o de dados (Terceira Forma Normal) para tabelas relacionais (`games`, `publishers`, `developers`).
3.  **Load (Carga):**
    * Modelagem Dimensional (Star Schema).
    * Carga na Tabela Fato (`fato_performance_steam`) e Dimens√µes (`dim_tempo`, `dim_jogo`, `dim_publisher`).

---

## üìã Pr√©-requisitos

Para rodar este projeto, voc√™ precisa apenas ter instalado na sua m√°quina:

* **[Docker Desktop](https://www.docker.com/products/docker-desktop/)**
* **Git** (Para clonar o reposit√≥rio)

> **Nota:** N√£o √© necess√°rio instalar Python ou Airflow localmente. O Docker cuidar√° de todas as depend√™ncias.

---

## üöÄ Como Executar (Passo a Passo)

Siga estas instru√ß√µes para subir o ambiente do zero.

### 1¬∞ Clone o Reposit√≥rio
Rode o comando:
```bash
git clone https://github.com/HenriqueIgreja/Steam-Games-Dataset.git
cd Steam-Games-Dataset
```


### 2¬∞ Configure o arquivo config.py
Verifique o arquivo dags/Scripts_normalizacao/DML/config.py. O projeto est√° configurado para rodar no Docker conectando-se ao PostgreSQL local.

Host: host.docker.internal (Padr√£o para Docker comunicar com Windows/Mac)

Database: postgres (Banco padr√£o)

Senha: Certifique se a senha no arquivo bate com a senha do seu banco local, ou ajuste conforme necess√°rio


### 3¬∞ Suba o Ambiente (Docker)
Com docker aberto, abra a raiz do projeto (onde est√° o docker-compose.yaml), e ent√£o execute:
```docker compose up -d```

### 4¬∞ Acesse o Airflow
Abra seu navegador e acesse:

URL:  [http://localhost:8080](http://localhost:8080)

Usu√°rio: ```airflow```

Senha: ```airflow```

### 5¬∞ Execute o Pipeline
Na lista de DAGs, procure por ```steam_etl_v1```

Ative a DAG clicando no interruptor na esquerda.

Clique no bot√£o play na direita e selecione "Trigger DAG".

### 6¬∞ Acompanhe o Processo
Clique no nome da DAG e v√° para a aba "Graph". Voc√™ ver√° as tarefas sendo executadas:

‚úÖ 0_instalar_libs: Instala depend√™ncias Python (ijson, psycopg2)

‚úÖ 0.5_criar_database: Cria o banco de dados automaticamente se n√£o existir

‚úÖ 1_infraestrutura_db: Cria/Recria as tabelas (OLTP e OLAP)

‚úÖ 2_ingestao_json: Popula o banco transacional

‚úÖ 3_carga_dw: Carrega o Data Warehouse


# Importa√ß√£o dos Dados
- Instale o Python 3.12+ 
- Clone o reposit√≥rio
- Crie um ambiente virtual na pasta raiz do projeto com o comando `py -m venv venv`
- Ative o ambiente com o comando `venv\Scripts\activate`
- Instale as libs necess√°rias com o comando `pip install psycopg2 ijson`
- Ajuste o arquivo config.py colocando sua senha corretamente.
- Rode o arquivo main.py
- Espere a importa√ß√£o de todos registros acabarem
- Pronto!
# üì¶ Entreg√°vel 1 ‚Äî Dicion√°rio de Dados Inicial (Conclu√≠do)

## Objetivo
Compreender completamente a estrutura atual da base de dados original antes de qualquer altera√ß√£o.

## Checklist
- [x] Analisar a base de dados original (sem modificar nada)
- [x] Listar todas as tabelas existentes
- [x] Documentar cada coluna contendo:
  - Tipo de dado
  - Descri√ß√£o
  - Observa√ß√µes relevantes
- [x] Identificar todas as chaves:
- [x] Prim√°rias
- [x] Estrangeiras
- [x] Criar o dicion√°rio de dados (Excel, Word ou PDF)

Arquivo .csv contendo o Dicion√°rio de Dados Inicial se encontra no caminho [Dicion√°rio_de_Dados_Inicial](/DW/Dicion√°rio_de_Dados_Inicial.csv).

# ‚öôÔ∏è Entreg√°vel 2 ‚Äî An√°lise da Base, Ajustes e Indexa√ß√£o (Conclu√≠do)

## Objetivo
Corrigir problemas estruturais, normalizar, ajustar rela√ß√µes e preparar um novo modelo consistente.

## Checklist
- [x] Identificar problemas da base:
  - [x] Falta de normaliza√ß√£o
  - [x] Rela√ß√µes mal definidas
  - [x] Estruturas inadequadas
  - [x] Tipos incorretos/inconsistentes
- [x] Propor todas as corre√ß√µes necess√°rias
- [x] Aplicar as corre√ß√µes no banco
- [x] Criar um script de migra√ß√£o da vers√£o antiga para a nova (preservando 100% dos dados)
- [x] Documentar e justificar cada modifica√ß√£o realizada
- [x] Criar √≠ndices para todas as tabelas
  - [x] Explicar utilidade dos √≠ndices para:
    - [x] Performance
    - [x] Integridade
    - [x] Consultas frequentes
- [x] Criar o novo dicion√°rio de dados

# üß© Entreg√°vel 3 ‚Äî Automatiza√ß√µes no PostgreSQL (Conclu√≠do)

## Objetivo
Criar automa√ß√µes significativas que agreguem valor ao dom√≠nio da base.

## Devem ser criados
- [x] 3 Triggers
- [x] 3 Functions
- [x] 3 Views
- [x] 3 Procedures

## Regras
- [x] Automatiza√ß√µes devem ser coerentes com o dom√≠nio
- [x] N√£o pode ser trivial (ex.: SELECT simples)
- [x] Cada automa√ß√£o deve ter justificativa explicando:
  - [x] Por que existe
  - [x] Qual problema resolve
  - [x] Como melhora o sistema
- [x] Adicionar nova se√ß√£o no novo dicion√°rio de dados

---

# üóÑÔ∏è Entreg√°vel 4 ‚Äî Modelagem do Data Warehouse (DW) (Conclu√≠do)

## Objetivo
Desenvolver o DW usando modelagem dimensional.

## Checklist
- [x] Escolher o tipo de modelagem (estrela, floco de neve etc.)
- [x] Criar pelo menos 1 tabela fato
- [x] Criar pelo menos 3 dimens√µes
- [x] Justificar o DW, explicando:
  - [x] Quais perguntas de neg√≥cio ele responde
  - [x] Qual valor anal√≠tico ele gera

# üîÑ Entreg√°vel 5 ‚Äî ETL para popular o DW (Conclu√≠do)

## Objetivo
Carregar o DW de forma automatizada utilizando uma ferramenta de ETL.

## Ferramentas (escolher uma)
- [ ] Apache NiFi
- [x] Apache Airflow
- [ ] Pentaho
- [ ] Kafka

## Checklist
- [x] Desenvolver o pipeline de ETL
- [x] Popular o DW automaticamente
- [x] Garantir que o processo seja reproduz√≠vel
- [x] Demonstrar o funcionamento do ETL

---

# ‚≠ê B√¥nus (opcional, mas vale nota extra)

## üéÅ B√¥nus 1 ‚Äî Backup Autom√°tico
- [ ] Implementar backup com:
  - [ ] pgBackRest  
  - [ ] ou pgBarman  

---

## üìä B√¥nus 2 ‚Äî Monitoramento do Banco

### Ferramentas poss√≠veis
- [ ] pgBadger
- [ ] TemBoard
- [ ] Prometheus + Grafana

### Checklist
- [ ] Implementar monitoramento
- [ ] Gerar consultas mal otimizadas
- [ ] Demonstrar nos dashboards:
  - [ ] Gargalos
  - [ ] Alertas
  - [ ] Problemas de performance
- [ ] Mostrar como o monitoramento auxilia na melhoria do banco

# üìà B√¥nus 3 ‚Äî Visualiza√ß√£o Anal√≠tica

- [ ] Criar dashboards usando Apache Superset com dados do DW

---

# üìå Observa√ß√µes Importantes

- Todas as entregas devem ser feitas pelo GitHub

## A avalia√ß√£o considerar√°:
- Commits de cada aluno
- Clareza no hist√≥rico do reposit√≥rio

## Cada aluno deve enviar:
-  Um v√≠deo de ~10 minutos explicando o que desenvolveu